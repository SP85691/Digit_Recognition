{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0GwKDlNSZoa"
      },
      "source": [
        "# **Introduction** <hr>\n",
        "- The MNIST (Modified National Institute of Standards and Technology) dataset is a widely used benchmark in the field of machine learning. It consists of a collection of handwritten digit images along with their corresponding labels. The MNIST model is a popular example of a deep learning model used to classify these digits accurately. This documentation provides a brief overview of the MNIST model.\n",
        "\n",
        "- MNIST consists of a large collection of handwritten digits from 0 to 9.\n",
        "\n",
        "- The dataset contains a training set of 60,000 grayscale images and a test set of 10,000 grayscale images. Each image is a 28x28 pixel square, representing a handwritten digit. The digits are centered and normalized, making the dataset relatively clean and consistent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI6e8k2ISowT"
      },
      "source": [
        "# **Step 1 : Importing the MNIST Dataset** <hr>\n",
        "- To begin working with the MNIST dataset in Python, you can utilize the TensorFlow library, which provides convenient access to the dataset. The following code snippet demonstrates how to import the MNIST dataset using TensorFlow.\n",
        "\n",
        "- By importing the MNIST dataset, you gain access to the training and testing subsets of handwritten digit images along with their corresponding labels. The dataset is divided into 60,000 training examples and 10,000 testing examples.\n",
        "\n",
        "- Next, you can proceed to preprocess and utilize this dataset to train a MNIST model for digit classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEN2l1F2RmWk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hEym5qaTd6o"
      },
      "source": [
        "# **Step 2 : Loading and Splitting the MNIST Dataset** <hr>\n",
        "- Once the MNIST dataset is imported, you can load and split it into training and testing subsets. The following code snippet demonstrates how to load and split the dataset using the **`mnist.load_data()`**\n",
        "\n",
        "- The X_train and X_test variables contain the image data, which are numpy arrays representing the grayscale pixel values of the handwritten digits. The shape of each image array is `(28, 28)`, indicating a 28x28-pixel image.\n",
        "\n",
        "- The Y_train and Y_test variables contain the corresponding labels for each image, indicating the true digit represented by the image. The labels are represented as integers ranging from 0 to 9.\n",
        "\n",
        "- By splitting the dataset into training and testing subsets, you can utilize the X_train and Y_train arrays to train your MNIST model, and then evaluate its performance using the X_test and Y_test arrays.\n",
        "\n",
        "- Next, you can proceed to preprocess the image data and build your MNIST model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ZX_1ExRu48"
      },
      "outputs": [],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iv2lJXxVukt"
      },
      "source": [
        "<h5 style=\"color: \t#FFFFFF;\"> **X_train, Y_train, X_test**, and **Y_test** represent the variables used to store the MNIST dataset after it has been loaded and split into training and testing sets. </h5> <hr>\n",
        "\n",
        "- **X_train**: This variable holds the training images data. (num_samples, height, width) <br>\n",
        "\n",
        "- **Y_train**: This variable contains the corresponding labels for the training images. `(num_samples)` Y_train represents the label (i.e., the digit from 0 to 9) corresponding to the respective image in X_train <br>\n",
        "\n",
        "- **X_test**: holds the testing images data.<br>\n",
        "\n",
        "- **Y_test**: contains the corresponding labels for the testing images. Each element in Y_test represents the label for the respective image in X_test <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q7kt2z5Tc0z"
      },
      "outputs": [],
      "source": [
        "print(\"The shape of X_train : \", X_train.shape)\n",
        "print(\"The shape of Y_train : \", Y_train.shape)\n",
        "print(\"The shape of X_test  : \", X_test.shape)\n",
        "print(\"The shape of Y_test  : \", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dryRQiLXtZa"
      },
      "source": [
        "### Showing the Visualized format of X_train datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1fnDAN9U-iP"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xSd3ayPX9cS"
      },
      "source": [
        "### Import **Matplotlib** Library and Visualize the X_train[0] data in Image Graphical Form.\n",
        "\n",
        "- We need to import the two libraries:\n",
        "```\n",
        "# Matplotlib\n",
        "# Numpy\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA4P8aKfX5T7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unB8j72LYumh"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0])\n",
        "plt.show()\n",
        "plt.imshow(np.invert(X_train[0]), cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcfyn8b8ZDu_"
      },
      "source": [
        "# **Step 3 - Normalizing the MNIST Dataset**\n",
        "- To improve the training process and model performance, it is often beneficial to normalize the pixel values of the MNIST dataset. Normalization scales the pixel values to a range of 0 to 1, making the data more suitable for training a neural network. The following code snippet demonstrates how to normalize the **MNIST** dataset using the **`tf.keras.utils.normalize()`** function\n",
        "```\n",
        "tf.keras.utils.normalize()\n",
        "```\n",
        "\n",
        "- In the above code, the **`tf.keras.utils.normalize()`** function is applied to both the X_train and X_test arrays. By specifying axis=1, the normalization is performed along the pixel values axis.\n",
        "\n",
        "- After normalization, the pixel values of the images are scaled to values between 0 and 1, ensuring that the neural network can effectively learn from the data.\n",
        "\n",
        "- Additionally, the **`plt.imshow()`** function from the matplotlib.pyplot library is used to visualize one of the normalized images from the X_train array. The cmap=plt.cm.binary argument sets the colormap to binary, displaying the image in black and white.\n",
        "\n",
        "- By normalizing the dataset, you have prepared the image data for training the MNIST model.\n",
        "\n",
        "- Next, you can proceed to build and train your MNIST model using the normalized dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znlg1bOWYOzx"
      },
      "outputs": [],
      "source": [
        "# Normalise the Datasets\n",
        "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
        "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
        "plt.imshow(X_train[0], cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zkhvmcffcW"
      },
      "source": [
        "## **After Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agU9rsL5aIfI"
      },
      "outputs": [],
      "source": [
        "# After Normalize\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h89Xuu5LfRMO"
      },
      "outputs": [],
      "source": [
        "print(Y_train[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jIVAdQ9Ef57Q"
      },
      "source": [
        "# **Step 4 - Resizing Images for Convolutional Operations**\n",
        "- To apply **convolutional operations** in a **CNN**, it is necessary to reshape the input images to include the channel dimension. The MNIST dataset consists of grayscale images, so the channel dimension will have a value of 1. The following code snippet demonstrates how to resize the **MNIST** images to the appropriate dimensions:\n",
        "    ```\n",
        "    import numpy as np\n",
        "    IMG_SIZE = 28\n",
        "    np.array(DATASET).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    ```\n",
        "\n",
        "- In the above code, the **`np.array()`** function is used to convert the X_train and X_test arrays into NumPy arrays. Then, the **`reshape()`** function is applied to each array to reshape the images.\n",
        "\n",
        "- The **`reshape()`** function takes in the following arguments:\n",
        "\n",
        "- **`-1`**: This indicates that the size of that dimension is inferred based on the other dimensions and the total number of elements.\n",
        "- **`IMG_SIZE`**: This is the desired image size after resizing.\n",
        "- **`IMG_SIZE`**: This is the desired image size after resizing.\n",
        "- **`1`**: This indicates the number of channels, which is 1 for grayscale images.\n",
        "- After reshaping, the dimensions of the training and testing samples are printed using the **`print()`** function.\n",
        "\n",
        "- Reshaping the images to include the channel dimension allows the CNN to process the images correctly during training and inference.\n",
        "\n",
        "- Next, you can proceed to build and train your MNIST model using the resized images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvZ86jkHfWa_"
      },
      "outputs": [],
      "source": [
        "# Resizing Image to make it suitable for applying Convolutional Operation\n",
        "import numpy as np\n",
        "IMG_SIZE = 28\n",
        "\n",
        "X_trainr = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "X_testr = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RLRmn2lhIj9"
      },
      "outputs": [],
      "source": [
        "print(\"Training Sample Dimension: \", X_trainr.shape)\n",
        "print(\"Testing Sample Dimension: \", X_testr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSU05womhWhU"
      },
      "source": [
        "# **Step 5 -  Creating a Deep Neural Network for MNIST Classification**\n",
        "\n",
        "- In this section, a deep neural network `(DNN)` model is created for classifying the MNIST handwritten digits. The model architecture includes multiple convolutional layers followed by **fully connected layers**. The following code snippet demonstrates how to create the DNN model:\n",
        "\n",
        "## **`1. Convolutional Layers:`**\n",
        "- The first convolutional layer takes an input shape of **`(28, 28, 1)`** (the shape of a single MNIST image) and applies 64 filters with a kernel size of **`(3, 3)`**. It uses the ReLU activation function to introduce non-linearity to the model and is followed by max pooling with a pool size of **`(2, 2)`**.\n",
        "\n",
        "- The second convolutional layer also applies 64 filters with a kernel size of **`(3, 3)`** and ReLU activation. It is followed by max pooling with the same pool size as the previous layer.\n",
        "\n",
        "- The third convolutional layer applies 64 filters with a kernel size of **`(3, 3)`** and ReLU activation. It is also followed by max pooling.\n",
        "\n",
        "\n",
        "## **`2. Flattening:`**\n",
        "- After the convolutional layers, the feature maps are flattened using the **`Flatten()`** layer. This transforms the 3-dimensional output into a 1-dimensional tensor, allowing it to be connected to the **fully connected layers**.\n",
        "\n",
        "## **`3. Fully Connected Layers:`**\n",
        "\n",
        "- The first fully connected layer consists of 64 units with ReLU activation. It takes the flattened feature maps as input and applies non-linearity to the model.\n",
        "\n",
        "- The second fully connected layer consists of `32 units` with `ReLU` activation.\n",
        "\n",
        "- The last fully connected layer has 10 units, corresponding to the 10 classes in the MNIST dataset **`(digits 0-9)`**. It uses the softmax activation function to produce a probability distribution over the classes.\n",
        "\n",
        "- The flow architecture of the MNIST model can be summarized as follows:\n",
        "\n",
        "```\n",
        "Input -> Convolutional Layers -> Flattening -> Fully Connected Layers -> Output\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRMaoVdqj43w"
      },
      "source": [
        "### **5.1. Import the dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qv5cHtWhJUT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXKj6acokHE-"
      },
      "source": [
        "### **5.2. Create a Model of Convolutional Neural Network `(CNN)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOi8blofkFHw"
      },
      "outputs": [],
      "source": [
        "# Create a Neural Network \n",
        "model = Sequential()\n",
        "\n",
        "# First Convolutional Layer ---> Layer 0 1 2 3 (60000, 28, 28, 1)\n",
        "model.add(Conv2D(64, (3,3), input_shape = X_trainr.shape[1:]))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Second Convolutional Layer ---> Layer 4 5 6 7 (60000, 28, 28, 1)\n",
        "model.add(Conv2D(64, (3,3)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Third Convolutional Layer ---> Layer 8 9 10 11 (60000, 28, 28, 1)\n",
        "model.add(Conv2D(64, (3,3)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Fully Connected Layer #1\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "# Fully Connected Layer #2\n",
        "model.add(Dense(32))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "# Fully Connected Layer #3\n",
        "model.add(Dense(32))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "# Last Fully Connected Layer \n",
        "model.add(Dense(10))\n",
        "model.add(Activation(\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hCxqiNBkkW2"
      },
      "source": [
        "### **5.3. Get the Summary of the Created Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0Y8fVU6ki2_"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbOWKZKlF7N"
      },
      "source": [
        "#### The summary provided gives an overview of the layers and parameters in the MNIST model. Here's a brief explanation of the summary:\n",
        "\n",
        "- The model is a Sequential model, meaning the layers are stacked sequentially.\n",
        "- The model consists of convolutional layers, activation layers, max pooling layers, a flattening layer, and dense layers.\n",
        "- The output shape of each layer represents the dimensions of the output tensor after passing through that layer.\n",
        "- The number of parameters indicates the total number of trainable parameters in the model.\n",
        "- The \"None\" dimension in the output shape indicates that it can vary depending on the batch size during training or inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz7v6asplQn0"
      },
      "source": [
        "## <u>**Detailed summary**</u>\n",
        "\n",
        "### **Convolutional Layer:**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Input shape**: **`(None, 28, 28, 1)`**\n",
        "- **Output shape**: **`(None, 26, 26, 64)`**\n",
        "- **Parameters**: **`40`**\n",
        "- **Activation** Layer: **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 26, 26, 64)`**\n",
        "- **Max Pooling Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **`Output shape`**: **`(None, 13, 13, 64)`**\n",
        "- **`Convolutional Layer:`** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 11, 11, 64)`**\n",
        "- **Parameters:** **`36,928`**\n",
        "- **Activation Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 11, 11, 64)`**\n",
        "- **Max Pooling Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 5, 5, 64)`**\n",
        "- **Convolutional Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 3, 3, 64)`**\n",
        "- **Parameters:** **`36,928`**\n",
        "- **Activation Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 3, 3, 64)`**\n",
        "- **Max Pooling Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 1, 1, 64)`**\n",
        "- **Flattening Layer:** **`None`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- **Output shape:** **`(None, 64)`**\n",
        "- **Dense Layer:** **`None`**\n",
        "- **Output shape:** **`(None, 64)`**\n",
        "- **Parameters:** **`4,160`**\n",
        "- **Activation Layer:** **`None`**\n",
        "- **Output shape:** **`(None, 64)`**\n",
        "- **Dense Layer:** **`None`**\n",
        "- **Output shape:** **`(None, 32)`**\n",
        "- **Parameters:** **`2,080`**\n",
        "- **Activation Layer:** **`None`**\n",
        "- **Output shape:** **`(None, 32)`**\n",
        "- **Dense Layer (Output Layer):** **`None`**\n",
        "- **Output shape:** **`(None, 10)`**\n",
        "- **Parameters:** **`330`**\n",
        "- **Activation Layer (Output Layer):** **`None`**\n",
        "- **Output shape:** **`(None, 10)`**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- The total number of trainable parameters in the model is **`81,066`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSkt4FBMo8xf"
      },
      "source": [
        "## **5.4. Compile the CNN Model**\n",
        "\n",
        "- The **`model.compile()`** function is used to configure the learning process of the model. It specifies the loss function, optimizer, and metrics to be used during training and evaluation. Here's an explanation of the arguments used in **`model.compile()`**:\n",
        "\n",
        "   - **loss:** The loss function measures the difference between the predicted output and the true output. In this case, `\"sparse_categorical_crossentropy\"` is used as the loss function. This loss function is suitable for multi-class classification problems where the labels are integers (e.g., the MNIST dataset contains integer labels representing digits from 0 to 9).\n",
        "\n",
        "   - **optimizer:** The optimizer determines how the model is updated based on the calculated gradients. The `\"adam\"` optimizer is used in this case. Adam `(Adaptive Moment Estimation)` is a popular optimizer that combines the benefits of two other optimizers, AdaGrad and RMSProp. It adapts the learning rate for each parameter, leading to efficient and effective optimization.\n",
        "\n",
        "   - **metrics:** Metrics are used to evaluate the performance of the model. The specified metrics are calculated and reported during training and evaluation. In this case, the model is evaluated based on the accuracy metric, which measures the proportion of correctly classified samples.\n",
        "\n",
        "- By calling **`model.compile()`** with the specified arguments, the model is prepared for training and evaluation. It sets up the necessary components to optimize the model's weights using the specified loss function and optimizer, and it tracks the specified metrics to assess the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeJAry6ykt0n"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpVfwY-9quDM"
      },
      "source": [
        "## **5.5. Fit the Model**\n",
        "- The **`model.fit()`** function is used to train the model on the provided training data. Here's a brief explanation of the arguments used in **`model.fit()`**:\n",
        "\n",
        "  - **X_trainr**: The training data, which includes the preprocessed and resized MNIST images as input to the model.\n",
        "\n",
        "  - **Y_train**: The corresponding labels for the training data, representing the true digits for each image.\n",
        "\n",
        "  - **epochs**: The number of epochs defines the number of times the model will iterate over the entire training dataset. In this case, the model will train for 5 epochs, meaning it will go through the entire training dataset 5 times during training.\n",
        "\n",
        "  - **validation_split**: The validation_split parameter specifies the fraction of the training data to be used for validation. In this case, 30% of the training data will be reserved for validation during training.\n",
        "\n",
        "- During the training process, the model adjusts its weights based on the training data and the specified loss function and optimizer. It aims to minimize the loss and improve its accuracy in predicting the correct digit labels.\n",
        "\n",
        "- The **`model.fit()`** function performs the training iterations **`(epochs)`** and reports the training progress, including the loss and accuracy metrics. It also evaluates the model's performance on the validation data at the end of each epoch.\n",
        "\n",
        "- By calling **`model.fit()`** with the provided arguments, the model will be trained on the MNIST training data for 5 epochs, with a portion of the data reserved for validation. The training progress and evaluation metrics will be displayed during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjKyhR7dqsCe"
      },
      "outputs": [],
      "source": [
        "model.fit(X_trainr, Y_train, epochs=5, validation_split=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdSsbCgKs2Z5"
      },
      "source": [
        "# **6. Evaluate the MNISt Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLLRs-fBq87N"
      },
      "outputs": [],
      "source": [
        "# Evaluating on Testing dataset MNIST\n",
        "test_loss, test_acc = model.evaluate(X_testr, Y_test)\n",
        "print(\"Test Loss on 10000 Test Samples: \", test_loss)\n",
        "print(\"Validation Accuracy on 10000 test samples: \", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BysuB0_QtAlh"
      },
      "source": [
        "# **7. Save The Model `(.h5)` Extension**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy0eG_kxs9SK"
      },
      "outputs": [],
      "source": [
        "model.save('MNIST_Model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ney9ebuDulMV"
      },
      "source": [
        "# **8. Test The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAjET3PAtYCh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def load_data():\n",
        "    # Load the MNIST dataset\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "    # Preprocess the data\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    # Reshape the data\n",
        "    X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
        "    X_test = np.reshape(X_test, (-1, 28, 28, 1))\n",
        "\n",
        "    return X_test, Y_test\n",
        "\n",
        "def load_model(model_file):\n",
        "    # Load the model from the file\n",
        "    model = tf.keras.models.load_model(model_file)\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, X_test, Y_test):\n",
        "    # Evaluate the model on the testing dataset\n",
        "    test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "\n",
        "    # Print the test loss and accuracy\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "\n",
        "    # Perform predictions on the testing dataset\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Convert predicted probabilities to class labels\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate and print classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(Y_test, predicted_labels))\n",
        "\n",
        "    # Plot a confusion matrix\n",
        "    cm = confusion_matrix(Y_test, predicted_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Load the testing data\n",
        "X_test, Y_test = load_data()\n",
        "\n",
        "# Specify the path to your model file\n",
        "model_file = \"MNIST_Model.h5\"\n",
        "\n",
        "# Load the model\n",
        "model = load_model(model_file)\n",
        "\n",
        "# Test the model\n",
        "test_model(model, X_test, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhIETOOVxrcL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Read the image\n",
        "image = cv2.imread('/content/drive/MyDrive/Digit_Patterns/Digits/Digits/eight_28.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "\n",
        "\n",
        "# Resize the image to match the input shape of your model\n",
        "image = cv2.resize(image, (28, 28))\n",
        "\n",
        "# Normalize the image\n",
        "image = image / 255.0\n",
        "\n",
        "# Reshape the image to match the input shape of your model\n",
        "image = np.reshape(image, (1, 28, 28, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG5Zpu9hy03p"
      },
      "outputs": [],
      "source": [
        "# Perform prediction on the external image\n",
        "prediction = model.predict(image)\n",
        "\n",
        "# Convert predicted probabilities to class label\n",
        "predicted_label = np.argmax(prediction[0])\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu4ftILsy14C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
